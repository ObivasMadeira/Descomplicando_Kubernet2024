O que é o kube-prometheus?

O kube-prometheus é um conjunto de manifestos do Kubernetes que nos permite ter o Prometheus Operator, Grafana, AlertManager, Node Exporter, Kube-State-Metrics, Prometheus-Adapter instalados e configurados de forma tranquila e com alta disponibilidade. Além disso, ele nos permite ter uma visão completa do nosso cluster de Kubernetes. Ele nos permite monitorar todos os componentes do nosso cluster de Kubernetes, como por exemplo: kube-scheduler, kube-controller-manager, kubelet, kube-proxy, etc.

===============================================================
Vamos criar nosso primeiro EKS.

eksctl create cluster --name=eks-cluster-ouri --version=1.28 --region=us-east-1 --nodegroup-name=eks-cluster-ouri-nodegroup --node-type=t3.medium --nodes=2 --nodes-min=1 --nodes-max=3 --managed
2025-05-06 23:03:11 [ℹ]  eksctl version 0.207.0
2025-05-06 23:03:11 [ℹ]  using region us-east-1
2025-05-06 23:03:12 [ℹ]  setting availability zones to [us-east-1f us-east-1a]
2025-05-06 23:03:12 [ℹ]  subnets for us-east-1f - public:192.168.0.0/19 private:192.168.64.0/19
2025-05-06 23:03:12 [ℹ]  subnets for us-east-1a - public:192.168.32.0/19 private:192.168.96.0/19
2025-05-06 23:03:12 [ℹ]  nodegroup "eks-cluster-ouri-nodegroup" will use "" [AmazonLinux2/1.28]
2025-05-06 23:03:12 [ℹ]  using Kubernetes version 1.28
2025-05-06 23:03:12 [ℹ]  creating EKS cluster "eks-cluster-ouri" in "us-east-1" region with managed nodes
2025-05-06 23:03:12 [ℹ]  will create 2 separate CloudFormation stacks for cluster itself and the initial managed nodegroup
2025-05-06 23:03:12 [ℹ]  if you encounter any issues, check CloudFormation console or try 'eksctl utils describe-stacks --region=us-east-1 --cluster=eks-cluster-ouri'
2025-05-06 23:03:12 [ℹ]  Kubernetes API endpoint access will use default of {publicAccess=true, privateAccess=false} for cluster "eks-cluster-ouri" in "us-east-1"
2025-05-06 23:03:12 [ℹ]  CloudWatch logging will not be enabled for cluster "eks-cluster-ouri" in "us-east-1"
2025-05-06 23:03:12 [ℹ]  you can enable it with 'eksctl utils update-cluster-logging --enable-types={SPECIFY-YOUR-LOG-TYPES-HERE (e.g. all)} --region=us-east-1 --cluster=eks-cluster-ouri'
2025-05-06 23:03:12 [ℹ]  default addons metrics-server, vpc-cni, kube-proxy, coredns were not specified, will install them as EKS addons
2025-05-06 23:03:12 [ℹ]  
2 sequential tasks: { create cluster control plane "eks-cluster-ouri", 
    2 sequential sub-tasks: { 
        2 sequential sub-tasks: { 
            1 task: { create addons },
            wait for control plane to become ready,
        },
        create managed nodegroup "eks-cluster-ouri-nodegroup",
    } 
}
2025-05-06 23:03:12 [ℹ]  building cluster stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:03:13 [ℹ]  deploying stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:03:43 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:04:14 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:05:15 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:06:15 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:07:16 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:08:17 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:09:17 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:10:18 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:11:19 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-cluster"
2025-05-06 23:11:22 [ℹ]  creating addon: metrics-server
2025-05-06 23:11:23 [ℹ]  successfully created addon: metrics-server
2025-05-06 23:11:24 [!]  recommended policies were found for "vpc-cni" addon, but since OIDC is disabled on the cluster, eksctl cannot configure the requested permissions; the recommended way to provide IAM permissions for "vpc-cni" addon is via pod identity associations; after addon creation is completed, add all recommended policies to the config file, under `addon.PodIdentityAssociations`, and run `eksctl update addon`
2025-05-06 23:11:24 [ℹ]  creating addon: vpc-cni
2025-05-06 23:11:24 [ℹ]  successfully created addon: vpc-cni
2025-05-06 23:11:25 [ℹ]  creating addon: kube-proxy
2025-05-06 23:11:26 [ℹ]  successfully created addon: kube-proxy
2025-05-06 23:11:26 [ℹ]  creating addon: coredns
2025-05-06 23:11:27 [ℹ]  successfully created addon: coredns
2025-05-06 23:13:30 [ℹ]  building managed nodegroup stack "eksctl-eks-cluster-ouri-nodegroup-eks-cluster-ouri-nodegroup"
2025-05-06 23:13:31 [ℹ]  deploying stack "eksctl-eks-cluster-ouri-nodegroup-eks-cluster-ouri-nodegroup"
2025-05-06 23:13:31 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-nodegroup-eks-cluster-ouri-nodegroup"
2025-05-06 23:14:02 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-nodegroup-eks-cluster-ouri-nodegroup"
2025-05-06 23:14:39 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-nodegroup-eks-cluster-ouri-nodegroup"
2025-05-06 23:15:19 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-nodegroup-eks-cluster-ouri-nodegroup"
2025-05-06 23:16:35 [ℹ]  waiting for CloudFormation stack "eksctl-eks-cluster-ouri-nodegroup-eks-cluster-ouri-nodegroup"
2025-05-06 23:16:35 [ℹ]  waiting for the control plane to become ready
2025-05-06 23:16:36 [✔]  saved kubeconfig as "/home/lucas_madeira/.kube/config"
2025-05-06 23:16:36 [ℹ]  no tasks
2025-05-06 23:16:36 [✔]  all EKS cluster resources for "eks-cluster-ouri" have been created
2025-05-06 23:16:36 [ℹ]  nodegroup "eks-cluster-ouri-nodegroup" has 2 node(s)
2025-05-06 23:16:36 [ℹ]  node "ip-192-168-3-107.ec2.internal" is ready
2025-05-06 23:16:36 [ℹ]  node "ip-192-168-57-198.ec2.internal" is ready
2025-05-06 23:16:36 [ℹ]  waiting for at least 1 node(s) to become ready in "eks-cluster-ouri-nodegroup"
2025-05-06 23:16:36 [ℹ]  nodegroup "eks-cluster-ouri-nodegroup" has 2 node(s)
2025-05-06 23:16:36 [ℹ]  node "ip-192-168-3-107.ec2.internal" is ready
2025-05-06 23:16:36 [ℹ]  node "ip-192-168-57-198.ec2.internal" is ready
2025-05-06 23:16:36 [✔]  created 1 managed nodegroup(s) in cluster "eks-cluster-ouri"
2025-05-06 23:16:38 [ℹ]  kubectl command should work with "/home/lucas_madeira/.kube/config", try 'kubectl get nodes'
2025-05-06 23:16:38 [✔]  EKS cluster "eks-cluster-ouri" in "us-east-1" region is ready

==============================================

Instalando o prometheus

git clone https://github.com/prometheus-operator/kube-prometheus.git
Cloning into 'kube-prometheus'...
remote: Enumerating objects: 21135, done.
remote: Counting objects: 100% (5721/5721), done.
remote: Compressing objects: 100% (318/318), done.
remote: Total 21135 (delta 5589), reused 5410 (delta 5400), pack-reused 15414 (from 3)
Receiving objects: 100% (21135/21135), 13.51 MiB | 5.43 MiB/s, done.
Resolving deltas: 100% (14690/14690), done.

kubectl create -f manifests/setup
customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusagents.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/scrapeconfigs.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
namespace/monitoring created

kubectl get namespace
NAME              STATUS   AGE
default           Active   32m
kube-node-lease   Active   32m
kube-public       Active   32m
kube-system       Active   32m
monitoring        Active   92s

kubectl apply -f manifests
alertmanager.monitoring.coreos.com/main created
networkpolicy.networking.k8s.io/alertmanager-main created
poddisruptionbudget.policy/alertmanager-main created
prometheusrule.monitoring.coreos.com/alertmanager-main-rules created
secret/alertmanager-main created
service/alertmanager-main created
serviceaccount/alertmanager-main created
servicemonitor.monitoring.coreos.com/alertmanager-main created
clusterrole.rbac.authorization.k8s.io/blackbox-exporter created
clusterrolebinding.rbac.authorization.k8s.io/blackbox-exporter created
configmap/blackbox-exporter-configuration created
deployment.apps/blackbox-exporter created
networkpolicy.networking.k8s.io/blackbox-exporter created
service/blackbox-exporter created
serviceaccount/blackbox-exporter created
servicemonitor.monitoring.coreos.com/blackbox-exporter created
secret/grafana-config created
secret/grafana-datasources created
configmap/grafana-dashboard-alertmanager-overview created
configmap/grafana-dashboard-apiserver created
configmap/grafana-dashboard-cluster-total created
configmap/grafana-dashboard-controller-manager created
configmap/grafana-dashboard-grafana-overview created
configmap/grafana-dashboard-k8s-resources-cluster created
configmap/grafana-dashboard-k8s-resources-multicluster created
configmap/grafana-dashboard-k8s-resources-namespace created
configmap/grafana-dashboard-k8s-resources-node created
configmap/grafana-dashboard-k8s-resources-pod created
configmap/grafana-dashboard-k8s-resources-windows-cluster created
configmap/grafana-dashboard-k8s-resources-windows-namespace created
configmap/grafana-dashboard-k8s-resources-windows-pod created
configmap/grafana-dashboard-k8s-resources-workload created
configmap/grafana-dashboard-k8s-resources-workloads-namespace created
configmap/grafana-dashboard-k8s-windows-cluster-rsrc-use created
configmap/grafana-dashboard-k8s-windows-node-rsrc-use created
configmap/grafana-dashboard-kubelet created
configmap/grafana-dashboard-namespace-by-pod created
configmap/grafana-dashboard-namespace-by-workload created
configmap/grafana-dashboard-node-cluster-rsrc-use created
configmap/grafana-dashboard-node-rsrc-use created
configmap/grafana-dashboard-nodes-aix created
configmap/grafana-dashboard-nodes-darwin created
configmap/grafana-dashboard-nodes created
configmap/grafana-dashboard-persistentvolumesusage created
configmap/grafana-dashboard-pod-total created
configmap/grafana-dashboard-prometheus-remote-write created
configmap/grafana-dashboard-prometheus created
configmap/grafana-dashboard-proxy created
configmap/grafana-dashboard-scheduler created
configmap/grafana-dashboard-workload-total created
configmap/grafana-dashboards created
deployment.apps/grafana created
networkpolicy.networking.k8s.io/grafana created
prometheusrule.monitoring.coreos.com/grafana-rules created
service/grafana created
serviceaccount/grafana created
servicemonitor.monitoring.coreos.com/grafana created
prometheusrule.monitoring.coreos.com/kube-prometheus-rules created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
deployment.apps/kube-state-metrics created
networkpolicy.networking.k8s.io/kube-state-metrics created
prometheusrule.monitoring.coreos.com/kube-state-metrics-rules created
service/kube-state-metrics created
serviceaccount/kube-state-metrics created
servicemonitor.monitoring.coreos.com/kube-state-metrics created
prometheusrule.monitoring.coreos.com/kubernetes-monitoring-rules created
servicemonitor.monitoring.coreos.com/kube-apiserver created
servicemonitor.monitoring.coreos.com/coredns created
servicemonitor.monitoring.coreos.com/kube-controller-manager created
servicemonitor.monitoring.coreos.com/kube-scheduler created
servicemonitor.monitoring.coreos.com/kubelet created
clusterrole.rbac.authorization.k8s.io/node-exporter created
clusterrolebinding.rbac.authorization.k8s.io/node-exporter created
daemonset.apps/node-exporter created
networkpolicy.networking.k8s.io/node-exporter created
prometheusrule.monitoring.coreos.com/node-exporter-rules created
service/node-exporter created
serviceaccount/node-exporter created
servicemonitor.monitoring.coreos.com/node-exporter created
clusterrole.rbac.authorization.k8s.io/prometheus-k8s created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created
networkpolicy.networking.k8s.io/prometheus-k8s created
poddisruptionbudget.policy/prometheus-k8s created
prometheus.monitoring.coreos.com/k8s created
prometheusrule.monitoring.coreos.com/prometheus-k8s-prometheus-rules created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s-config created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
service/prometheus-k8s created
serviceaccount/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus-k8s created
Warning: resource apiservices/v1beta1.metrics.k8s.io is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io configured
clusterrole.rbac.authorization.k8s.io/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created
clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created
clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created
configmap/adapter-config created
deployment.apps/prometheus-adapter created
networkpolicy.networking.k8s.io/prometheus-adapter created
poddisruptionbudget.policy/prometheus-adapter created
rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created
service/prometheus-adapter created
serviceaccount/prometheus-adapter created
servicemonitor.monitoring.coreos.com/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
networkpolicy.networking.k8s.io/prometheus-operator created
prometheusrule.monitoring.coreos.com/prometheus-operator-rules created
service/prometheus-operator created
serviceaccount/prometheus-operator created
servicemonitor.monitoring.coreos.com/prometheus-operator created

kubectl get pods -n monitoring
NAME                                   READY   STATUS    RESTARTS   AGE
alertmanager-main-0                    2/2     Running   0          30s
alertmanager-main-1                    2/2     Running   0          30s
alertmanager-main-2                    2/2     Running   0          30s
blackbox-exporter-7f7b644956-47f5x     3/3     Running   0          88s
grafana-745fd45b68-l2ht6               1/1     Running   0          65s
kube-state-metrics-66d5985456-dszhq    3/3     Running   0          61s
node-exporter-khk4z                    2/2     Running   0          54s
node-exporter-x4v2x                    2/2     Running   0          54s
prometheus-adapter-77f8587965-rbxtb    1/1     Running   0          40s
prometheus-adapter-77f8587965-wqm9j    1/1     Running   0          40s
prometheus-k8s-0                       2/2     Running   0          30s
prometheus-k8s-1                       2/2     Running   0          30s
prometheus-operator-65c5ff896f-7wfkg   2/2     Running   0          36s
kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.100.169.117   <none>        9093/TCP,8080/TCP            114s
alertmanager-operated   ClusterIP   None             <none>        9093/TCP,9094/TCP,9094/UDP   50s
blackbox-exporter       ClusterIP   10.100.200.232   <none>        9115/TCP,19115/TCP           110s
grafana                 ClusterIP   10.100.12.103    <none>        3000/TCP                     86s
kube-state-metrics      ClusterIP   None             <none>        8443/TCP,9443/TCP            82s
node-exporter           ClusterIP   None             <none>        9100/TCP                     76s
prometheus-adapter      ClusterIP   10.100.58.24     <none>        443/TCP                      61s
prometheus-k8s          ClusterIP   10.100.144.36    <none>        9090/TCP,8080/TCP            67s
prometheus-operated     ClusterIP   None             <none>        9090/TCP                     50s
prometheus-operator     ClusterIP   None             <none>        8443/TCP                     57s
kubectl get servicemonitor -n monitoring
NAME                      AGE
alertmanager-main         2m
blackbox-exporter         117s
coredns                   88s
grafana                   93s
kube-apiserver            88s
kube-controller-manager   87s
kube-scheduler            87s
kube-state-metrics        89s
kubelet                   86s
node-exporter             83s
prometheus-adapter        68s
prometheus-k8s            74s
prometheus-operator       64s

==================================
Comando importantes

kubectl config get-contexts

kubectl config get-contexts
CURRENT   NAME                                                    CLUSTER                                AUTHINFO                                                NAMESPACE
*         iam-root-account@eks-cluster-ouri.us-east-1.eksctl.io   eks-cluster-ouri.us-east-1.eksctl.io   iam-root-account@eks-cluster-ouri.us-east-1.eksctl.io   
          kind-kind                                               kind-kind                              kind-kind                                               

kubectl config use-context kind-kind
Switched to context "kind-kind".

======================================
INstalando kubeprometheus

Primeiro clonamos o repo do projeto

git clone https://github.com/prometheus-operator/kube-prometheus.git
Cloning into 'kube-prometheus'...
remote: Enumerating objects: 21148, done.
remote: Counting objects: 100% (5334/5334), done.
remote: Compressing objects: 100% (282/282), done.
remote: Total 21148 (delta 5228), reused 5052 (delta 5052), pack-reused 15814 (from 4)
Receiving objects: 100% (21148/21148), 13.54 MiB | 4.97 MiB/s, done.
Resolving deltas: 100% (14696/14696), done.

Após clonar executar o primeiro serviço

kubectl create -f manifests/setup
customresourcedefinition.apiextensions.k8s.io/alertmanagerconfigs.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/alertmanagers.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/podmonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/probes.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheuses.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusagents.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/prometheusrules.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/scrapeconfigs.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/servicemonitors.monitoring.coreos.com created
customresourcedefinition.apiextensions.k8s.io/thanosrulers.monitoring.coreos.com created
namespace/monitoring created

Depois o seguinte:

kubectl apply -f manifests
alertmanager.monitoring.coreos.com/main created
networkpolicy.networking.k8s.io/alertmanager-main created
poddisruptionbudget.policy/alertmanager-main created
prometheusrule.monitoring.coreos.com/alertmanager-main-rules created
secret/alertmanager-main created
service/alertmanager-main created
serviceaccount/alertmanager-main created
servicemonitor.monitoring.coreos.com/alertmanager-main created
clusterrole.rbac.authorization.k8s.io/blackbox-exporter created
clusterrolebinding.rbac.authorization.k8s.io/blackbox-exporter created
configmap/blackbox-exporter-configuration created
deployment.apps/blackbox-exporter created
networkpolicy.networking.k8s.io/blackbox-exporter created
service/blackbox-exporter created
serviceaccount/blackbox-exporter created
servicemonitor.monitoring.coreos.com/blackbox-exporter created
secret/grafana-config created
secret/grafana-datasources created
configmap/grafana-dashboard-alertmanager-overview created
configmap/grafana-dashboard-apiserver created
configmap/grafana-dashboard-cluster-total created
configmap/grafana-dashboard-controller-manager created
configmap/grafana-dashboard-grafana-overview created
configmap/grafana-dashboard-k8s-resources-cluster created
configmap/grafana-dashboard-k8s-resources-multicluster created
configmap/grafana-dashboard-k8s-resources-namespace created
configmap/grafana-dashboard-k8s-resources-node created
configmap/grafana-dashboard-k8s-resources-pod created
configmap/grafana-dashboard-k8s-resources-windows-cluster created
configmap/grafana-dashboard-k8s-resources-windows-namespace created
configmap/grafana-dashboard-k8s-resources-windows-pod created
configmap/grafana-dashboard-k8s-resources-workload created
configmap/grafana-dashboard-k8s-resources-workloads-namespace created
configmap/grafana-dashboard-k8s-windows-cluster-rsrc-use created
configmap/grafana-dashboard-k8s-windows-node-rsrc-use created
configmap/grafana-dashboard-kubelet created
configmap/grafana-dashboard-namespace-by-pod created
configmap/grafana-dashboard-namespace-by-workload created
configmap/grafana-dashboard-node-cluster-rsrc-use created
configmap/grafana-dashboard-node-rsrc-use created
configmap/grafana-dashboard-nodes-aix created
configmap/grafana-dashboard-nodes-darwin created
configmap/grafana-dashboard-nodes created
configmap/grafana-dashboard-persistentvolumesusage created
configmap/grafana-dashboard-pod-total created
configmap/grafana-dashboard-prometheus-remote-write created
configmap/grafana-dashboard-prometheus created
configmap/grafana-dashboard-proxy created
configmap/grafana-dashboard-scheduler created
configmap/grafana-dashboard-workload-total created
configmap/grafana-dashboards created
deployment.apps/grafana created
networkpolicy.networking.k8s.io/grafana created
prometheusrule.monitoring.coreos.com/grafana-rules created
service/grafana created
serviceaccount/grafana created
servicemonitor.monitoring.coreos.com/grafana created
prometheusrule.monitoring.coreos.com/kube-prometheus-rules created
clusterrole.rbac.authorization.k8s.io/kube-state-metrics created
clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created
deployment.apps/kube-state-metrics created
networkpolicy.networking.k8s.io/kube-state-metrics created
prometheusrule.monitoring.coreos.com/kube-state-metrics-rules created
service/kube-state-metrics created
serviceaccount/kube-state-metrics created
servicemonitor.monitoring.coreos.com/kube-state-metrics created
prometheusrule.monitoring.coreos.com/kubernetes-monitoring-rules created
servicemonitor.monitoring.coreos.com/kube-apiserver created
servicemonitor.monitoring.coreos.com/coredns created
servicemonitor.monitoring.coreos.com/kube-controller-manager created
servicemonitor.monitoring.coreos.com/kube-scheduler created
servicemonitor.monitoring.coreos.com/kubelet created
clusterrole.rbac.authorization.k8s.io/node-exporter created
clusterrolebinding.rbac.authorization.k8s.io/node-exporter created
daemonset.apps/node-exporter created
networkpolicy.networking.k8s.io/node-exporter created
prometheusrule.monitoring.coreos.com/node-exporter-rules created
service/node-exporter created
serviceaccount/node-exporter created
servicemonitor.monitoring.coreos.com/node-exporter created
clusterrole.rbac.authorization.k8s.io/prometheus-k8s created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-k8s created
networkpolicy.networking.k8s.io/prometheus-k8s created
poddisruptionbudget.policy/prometheus-k8s created
prometheus.monitoring.coreos.com/k8s created
prometheusrule.monitoring.coreos.com/prometheus-k8s-prometheus-rules created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s-config created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
rolebinding.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s-config created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
role.rbac.authorization.k8s.io/prometheus-k8s created
service/prometheus-k8s created
serviceaccount/prometheus-k8s created
servicemonitor.monitoring.coreos.com/prometheus-k8s created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
clusterrole.rbac.authorization.k8s.io/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-adapter created
clusterrolebinding.rbac.authorization.k8s.io/resource-metrics:system:auth-delegator created
clusterrole.rbac.authorization.k8s.io/resource-metrics-server-resources created
configmap/adapter-config created
deployment.apps/prometheus-adapter created
networkpolicy.networking.k8s.io/prometheus-adapter created
poddisruptionbudget.policy/prometheus-adapter created
rolebinding.rbac.authorization.k8s.io/resource-metrics-auth-reader created
service/prometheus-adapter created
serviceaccount/prometheus-adapter created
servicemonitor.monitoring.coreos.com/prometheus-adapter created
clusterrole.rbac.authorization.k8s.io/prometheus-operator created
clusterrolebinding.rbac.authorization.k8s.io/prometheus-operator created
deployment.apps/prometheus-operator created
networkpolicy.networking.k8s.io/prometheus-operator created
prometheusrule.monitoring.coreos.com/prometheus-operator-rules created
service/prometheus-operator created
serviceaccount/prometheus-operator created
servicemonitor.monitoring.coreos.com/prometheus-operator created
kubectl get pods -n monitoring
NAME                                  READY   STATUS    RESTARTS   AGE
alertmanager-main-0                   2/2     Running   0          4m27s
alertmanager-main-1                   2/2     Running   0          4m27s
alertmanager-main-2                   2/2     Running   0          4m27s
blackbox-exporter-d4d45d65-97rsc      3/3     Running   0          5m11s
grafana-68958fc7b9-prc7n              1/1     Running   0          5m11s
kube-state-metrics-6d8b8c455-hr4xk    3/3     Running   0          5m10s
node-exporter-2klvg                   2/2     Running   0          5m10s
node-exporter-95wr5                   2/2     Running   0          5m10s
node-exporter-wth72                   2/2     Running   0          5m10s
node-exporter-zdbb9                   2/2     Running   0          5m10s
prometheus-adapter-784f566c54-bt822   1/1     Running   0          5m9s
prometheus-adapter-784f566c54-z6j6t   1/1     Running   0          5m9s
prometheus-k8s-0                      2/2     Running   0          4m27s
prometheus-k8s-1                      2/2     Running   0          4m26s
prometheus-operator-7cd6d5b6b-xvwm8   2/2     Running   0          5m9s
kubectl get svc -n monitoring
NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
alertmanager-main       ClusterIP   10.96.227.212   <none>        9093/TCP,8080/TCP            5m17s
alertmanager-operated   ClusterIP   None            <none>        9093/TCP,9094/TCP,9094/UDP   4m33s
blackbox-exporter       ClusterIP   10.96.140.82    <none>        9115/TCP,19115/TCP           5m17s
grafana                 ClusterIP   10.96.163.156   <none>        3000/TCP                     5m17s
kube-state-metrics      ClusterIP   None            <none>        8443/TCP,9443/TCP            5m16s
node-exporter           ClusterIP   None            <none>        9100/TCP                     5m16s
prometheus-adapter      ClusterIP   10.96.103.44    <none>        443/TCP                      5m15s
prometheus-k8s          ClusterIP   10.96.62.176    <none>        9090/TCP,8080/TCP            5m16s
prometheus-operated     ClusterIP   None            <none>        9090/TCP                     4m33s
prometheus-operator     ClusterIP   None            <none>        8443/TCP                     5m15s
kubectl get servicemonitor -n monitoring
NAME                      AGE
alertmanager-main         5m20s
blackbox-exporter         5m20s
coredns                   5m19s
grafana                   5m20s
kube-apiserver            5m19s
kube-controller-manager   5m19s
kube-scheduler            5m19s
kube-state-metrics        5m19s
kubelet                   5m19s
node-exporter             5m19s
prometheus-adapter        5m18s
prometheus-k8s            5m19s
prometheus-operator       5m18s

==================================

kubectl get all -n monitoring
NAME                                      READY   STATUS    RESTARTS   AGE
pod/alertmanager-main-0                   2/2     Running   0          8m8s
pod/alertmanager-main-1                   2/2     Running   0          8m8s
pod/alertmanager-main-2                   2/2     Running   0          8m8s
pod/blackbox-exporter-d4d45d65-97rsc      3/3     Running   0          8m52s
pod/grafana-68958fc7b9-prc7n              1/1     Running   0          8m52s
pod/kube-state-metrics-6d8b8c455-hr4xk    3/3     Running   0          8m51s
pod/node-exporter-2klvg                   2/2     Running   0          8m51s
pod/node-exporter-95wr5                   2/2     Running   0          8m51s
pod/node-exporter-wth72                   2/2     Running   0          8m51s
pod/node-exporter-zdbb9                   2/2     Running   0          8m51s
pod/prometheus-adapter-784f566c54-bt822   1/1     Running   0          8m50s
pod/prometheus-adapter-784f566c54-z6j6t   1/1     Running   0          8m50s
pod/prometheus-k8s-0                      2/2     Running   0          8m8s
pod/prometheus-k8s-1                      2/2     Running   0          8m7s
pod/prometheus-operator-7cd6d5b6b-xvwm8   2/2     Running   0          8m50s

NAME                            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                      AGE
service/alertmanager-main       ClusterIP   10.96.227.212   <none>        9093/TCP,8080/TCP            8m52s
service/alertmanager-operated   ClusterIP   None            <none>        9093/TCP,9094/TCP,9094/UDP   8m8s
service/blackbox-exporter       ClusterIP   10.96.140.82    <none>        9115/TCP,19115/TCP           8m52s
service/grafana                 ClusterIP   10.96.163.156   <none>        3000/TCP                     8m52s
service/kube-state-metrics      ClusterIP   None            <none>        8443/TCP,9443/TCP            8m51s
service/node-exporter           ClusterIP   None            <none>        9100/TCP                     8m51s
service/prometheus-adapter      ClusterIP   10.96.103.44    <none>        443/TCP                      8m50s
service/prometheus-k8s          ClusterIP   10.96.62.176    <none>        9090/TCP,8080/TCP            8m51s
service/prometheus-operated     ClusterIP   None            <none>        9090/TCP                     8m8s
service/prometheus-operator     ClusterIP   None            <none>        8443/TCP                     8m50s

NAME                           DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
daemonset.apps/node-exporter   4         4         4       4            4           kubernetes.io/os=linux   8m51s

NAME                                  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/blackbox-exporter     1/1     1            1           8m52s
deployment.apps/grafana               1/1     1            1           8m52s
deployment.apps/kube-state-metrics    1/1     1            1           8m51s
deployment.apps/prometheus-adapter    2/2     2            2           8m51s
deployment.apps/prometheus-operator   1/1     1            1           8m50s

NAME                                            DESIRED   CURRENT   READY   AGE
replicaset.apps/blackbox-exporter-d4d45d65      1         1         1       8m52s
replicaset.apps/grafana-68958fc7b9              1         1         1       8m52s
replicaset.apps/kube-state-metrics-6d8b8c455    1         1         1       8m51s
replicaset.apps/prometheus-adapter-784f566c54   2         2         2       8m51s
replicaset.apps/prometheus-operator-7cd6d5b6b   1         1         1       8m50s

NAME                                 READY   AGE
statefulset.apps/alertmanager-main   3/3     8m8s
statefulset.apps/prometheus-k8s      2/2     8m8s


Depos de instalar td, como vamos acessar os services? já que os mesmo estão ClusterIP?
(Podemos criar uma regra no ingress ou um service nodeport) no nosso caso não faremos isso agora.
Usaremos o seguinte comando
kubectl port-forward -n monitoring svc/grafana 33000:3000


kubectl get services -A
NAMESPACE     NAME                    TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)                        AGE
default       kubernetes              ClusterIP   10.96.0.1       <none>        443/TCP                        124d
default       nginx-certificado       ClusterIP   10.96.184.203   <none>        80/TCP,443/TCP                 6d
kube-system   kube-dns                ClusterIP   10.96.0.10      <none>        53/UDP,53/TCP,9153/TCP         124d
kube-system   kubelet                 ClusterIP   None            <none>        10250/TCP,10255/TCP,4194/TCP   20m
monitoring    alertmanager-main       ClusterIP   10.96.227.212   <none>        9093/TCP,8080/TCP              21m
monitoring    alertmanager-operated   ClusterIP   None            <none>        9093/TCP,9094/TCP,9094/UDP     20m
monitoring    blackbox-exporter       ClusterIP   10.96.140.82    <none>        9115/TCP,19115/TCP             21m
monitoring    grafana                 ClusterIP   10.96.163.156   <none>        3000/TCP                       21m
monitoring    kube-state-metrics      ClusterIP   None            <none>        8443/TCP,9443/TCP              21m
monitoring    node-exporter           ClusterIP   None            <none>        9100/TCP                       21m
monitoring    prometheus-adapter      ClusterIP   10.96.103.44    <none>        443/TCP                        21m
monitoring    prometheus-k8s          ClusterIP   10.96.62.176    <none>        9090/TCP,8080/TCP              21m
monitoring    prometheus-operated     ClusterIP   None            <none>        9090/TCP                       20m
monitoring    prometheus-operator     ClusterIP   None            <none>        8443/TCP                       21m

===========================================================
Subimos para analisar tb o prometheus e o alertmanager

kubectl port-forward -n monitoring svc/prometheus-k8s 39090:9090
kubectl port-forward -n monitoring svc/alertmanager-main 39093:9093
 
================================================================
Servicemonitor

kubectl get servicemonitor -n monitoring grafana -o yaml
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"monitoring.coreos.com/v1","kind":"ServiceMonitor","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"grafana","app.kubernetes.io/name":"grafana","app.kubernetes.io/part-of":"kube-prometheus","app.kubernetes.io/version":"12.0.0"},"name":"grafana","namespace":"monitoring"},"spec":{"endpoints":[{"interval":"15s","port":"http"}],"selector":{"matchLabels":{"app.kubernetes.io/name":"grafana"}}}}
  creationTimestamp: "2025-05-13T00:58:20Z"
  generation: 1
  labels:
    app.kubernetes.io/component: grafana
    app.kubernetes.io/name: grafana
    app.kubernetes.io/part-of: kube-prometheus
    app.kubernetes.io/version: 12.0.0
  name: grafana
  namespace: monitoring
  resourceVersion: "9128832"
  uid: 6d72e8ca-97b3-4e38-9820-d89d3a82d55d
spec:
  endpoints:
  - interval: 15s
    port: http
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana

====================================================================

Um dos principais recursos que o Kube-Prometheus utiliza é o ServiceMonitor. O ServiceMonitor é um recurso do Prometheus Operator que permite que você configure o Prometheus para monitorar um serviço. Para isso, você precisa criar um ServiceMonitor para cada serviço que você deseja monitorar.

O Kube-Prometheus já vem com vários ServiceMonitors configurados, como por exemplo o ServiceMonitor do API Server, do Node Exporter, do Blackbox Exporter, etc.

kubectl get servicemonitors -n monitoring
NAME                      AGE
alertmanager              17m
blackbox-exporter         17m
coredns                   17m
grafana                   17m
kube-apiserver            17m
kube-controller-manager   17m
kube-scheduler            17m
kube-state-metrics        17m
kubelet                   17m
node-exporter             17m
prometheus-adapter        17m
prometheus-k8s            17m
prometheus-operator       17m
Para ver o conteúdo de um ServiceMonitor, basta executar o seguinte comando:

kubectl get servicemonitor prometheus-k8s -n monitoring -o yaml
Nesse caso estamos pegando o ServiceMonitor do Prometheus, mas você pode pegar o ServiceMonitor de qualquer outro serviço.

apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"monitoring.coreos.com/v1","kind":"ServiceMonitor","metadata":{"annotations":{},"labels":{"app.kubernetes.io/component":"prometheus","app.kubernetes.io/instance":"k8s","app.kubernetes.io/name":"prometheus","app.kubernetes.io/part-of":"kube-prometheus","app.kubernetes.io/version":"2.41.0"},"name":"prometheus-k8s","namespace":"monitoring"},"spec":{"endpoints":[{"interval":"30s","port":"web"},{"interval":"30s","port":"reloader-web"}],"selector":{"matchLabels":{"app.kubernetes.io/component":"prometheus","app.kubernetes.io/instance":"k8s","app.kubernetes.io/name":"prometheus","app.kubernetes.io/part-of":"kube-prometheus"}}}}
  creationTimestamp: "2023-01-23T19:08:26Z"
  generation: 1
  labels:
    app.kubernetes.io/component: prometheus
    app.kubernetes.io/instance: k8s
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/part-of: kube-prometheus
    app.kubernetes.io/version: 2.41.0
  name: prometheus-k8s
  namespace: monitoring
  resourceVersion: "4100"
  uid: 6042e08c-cf18-4622-9860-3ff43e696f7c
spec:
  endpoints:
  - interval: 30s
    port: web
  - interval: 30s
    port: reloader-web
  selector:
    matchLabels:
      app.kubernetes.io/component: prometheus
      app.kubernetes.io/instance: k8s
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: kube-prometheus
Eu vou dar uma limpada nessa saída para ficar mais fácil de entender:

apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  annotations:
  labels:
    app.kubernetes.io/component: prometheus
    app.kubernetes.io/instance: k8s
    app.kubernetes.io/name: prometheus
    app.kubernetes.io/part-of: kube-prometheus
    app.kubernetes.io/version: 2.41.0
  name: prometheus-k8s
  namespace: monitoring
spec:
  endpoints:
  - interval: 30s
    port: web
  - interval: 30s
    port: reloader-web
  selector:
    matchLabels:
      app.kubernetes.io/component: prometheus
      app.kubernetes.io/instance: k8s
      app.kubernetes.io/name: prometheus
      app.kubernetes.io/part-of: kube-prometheus
Pronto, eu tirei algumas informações que não são importantes para a criação do ServiceMonitor, elas apenas trazer as informações do service monitor que foi criado e que pegamos a saída.

Com o arquivo limpo, podemos entender melhor o que está acontecendo.

apiVersion: Versão da API do Kubernetes que estamos utilizando.
kind: Tipo de objeto que estamos criando.
metadata: Informações sobre o objeto que estamos criando.
metadata.annotations: Anotações que podemos adicionar ao nosso objeto.
metadata.labels: Labels que podemos adicionar ao nosso objeto.
metadata.name: Nome do nosso objeto.
metadata.namespace: Namespace onde o nosso objeto será criado.
spec: Especificações do nosso objeto.
spec.endpoints: Endpoints que o nosso ServiceMonitor irá monitorar.
spec.endpoints.interval: Intervalo de tempo que o Prometheus irá fazer a coleta de métricas.
spec.endpoints.port: Porta que o Prometheus irá utilizar para coletar as métricas.
spec.selector: Selector que o ServiceMonitor irá utilizar para encontrar os serviços que ele irá monitorar.
Com isso, sabemos que o ServiceMonitor do Prometheus irá monitorar os serviços que possuem as labels app.kubernetes.io/component: prometheus, app.kubernetes.io/instance: k8s, app.kubernetes.io/name: prometheus e app.kubernetes.io/part-of: kube-prometheus, e que ele irá monitorar as portas web e reloader-web com um intervalo de 30 segundos. É fácil ou não é?

Então sempre que precisarmos criar um ServiceMonitor para monitorar algum serviço, basta criarmos um arquivo YAML com as informações que precisamos e aplicarmos em nosso cluster.

====================================================================

19/05

Continuação servicemonitor

kubectl get customresourcedefinitions.apiextensions.k8s.io

kubectl get customresourcedefinitions.apiextensions.k8s.io
NAME                                        CREATED AT
alertmanagerconfigs.monitoring.coreos.com   2025-05-13T00:49:22Z
alertmanagers.monitoring.coreos.com         2025-05-13T00:49:23Z
authorizationpolicies.security.istio.io     2025-05-13T14:26:12Z
destinationrules.networking.istio.io        2025-05-13T14:26:11Z
envoyfilters.networking.istio.io            2025-05-13T14:26:11Z
gatewayclasses.gateway.networking.k8s.io    2025-05-13T14:29:04Z
gateways.gateway.networking.k8s.io          2025-05-13T14:29:04Z
gateways.networking.istio.io                2025-05-13T14:26:11Z
grpcroutes.gateway.networking.k8s.io        2025-05-13T14:29:05Z
httproutes.gateway.networking.k8s.io        2025-05-13T14:29:05Z
peerauthentications.security.istio.io       2025-05-13T14:26:12Z
podmonitors.monitoring.coreos.com           2025-05-13T00:49:23Z
probes.monitoring.coreos.com                2025-05-13T00:49:23Z
prometheusagents.monitoring.coreos.com      2025-05-13T00:49:23Z
prometheuses.monitoring.coreos.com          2025-05-13T00:49:23Z
prometheusrules.monitoring.coreos.com       2025-05-13T00:49:24Z
proxyconfigs.networking.istio.io            2025-05-13T14:26:11Z
referencegrants.gateway.networking.k8s.io   2025-05-13T14:29:05Z
requestauthentications.security.istio.io    2025-05-13T14:26:12Z
scrapeconfigs.monitoring.coreos.com         2025-05-13T00:49:24Z
serviceentries.networking.istio.io          2025-05-13T14:26:11Z
servicemonitors.monitoring.coreos.com       2025-05-13T00:49:24Z
sidecars.networking.istio.io                2025-05-13T14:26:11Z
telemetries.telemetry.istio.io              2025-05-13T14:26:12Z
thanosrulers.monitoring.coreos.com          2025-05-13T00:49:24Z
virtualservices.networking.istio.io         2025-05-13T14:26:11Z
wasmplugins.extensions.istio.io             2025-05-13T14:26:11Z
workloadentries.networking.istio.io         2025-05-13T14:26:11Z
workloadgroups.networking.istio.io          2025-05-13T14:26:12Z

================================================

Criando um ServiceMonitor
Bem, chegou a hora de criarmos o nosso primeiro ServiceMonitor, mas antes precisamos ter uma aplicação para monitorarmos. Para isso, vamos criar uma aplicação com Nginx e utilizar o exporter do Nginx para monitorarmos o nosso serviço.
Vamos criar ainda um outro pod para que possamos criar um teste de carga para a nossa aplicação, realizando assim uma carga de até 1000 requisições por segundo.

Antes de mais nada, precisamos criar um ConfigMap onde terá a configuração que queremos para o nosso Nginx. Nada demais, somente a criação do nosso ConfigMap com o arquivo de configuração do Nginx, onde vamos definir a rota /nginx_status para expor as métricas do Nginx, além de expor a rota /metrics para expor as métricas do Nginx Exporter.

Vamos criar o nosso ConfigMap com o seguinte arquivo YAML:

apiVersion: v1 # versão da API
kind: ConfigMap # tipo de recurso, no caso, um ConfigMap
metadata: # metadados do recurso
  name: nginx-config # nome do recurso
data: # dados do recurso
  nginx.conf: | # inicio da definição do arquivo de configuração do Nginx
    server {
      listen 80;
      location / {
        root /usr/share/nginx/html;
        index index.html index.htm;
      }
      location /metrics {
        stub_status on;
        access_log off;
      }
    }
Agora vamos entender o que está acontecendo no nosso arquivo YAML.

apiVersion: Versão da API do Kubernetes que estamos utilizando.
kind: Tipo de objeto que estamos criando.
metadata: Informações sobre o objeto que estamos criando.
metadata.name: Nome do nosso objeto.
data: Dados que serão utilizados no nosso ConfigMap.
data.nginx.conf: A configuração do Nginx.
Vamos criar o nosso ConfigMap com o seguinte comando:

kubectl apply -f nginx-config.yaml
Após o nosso ConfigMap ser criado, vamos verificar se o nosso ConfigMap está rodando:

kubectl get configmaps
Para criar a nossa aplicação, vamos utilizar o seguinte arquivo YAML:

apiVersion: apps/v1 # versão da API
kind: Deployment # tipo de recurso, no caso, um Deployment
metadata: # metadados do recurso 
  name: nginx-server # nome do recurso
spec: # especificação do recurso
  selector: # seletor para identificar os pods que serão gerenciados pelo deployment
    matchLabels: # labels que identificam os pods que serão gerenciados pelo deployment
      app: nginx # label que identifica o app que será gerenciado pelo deployment
  replicas: 3 # quantidade de réplicas do deployment
  template: # template do deployment
    metadata: # metadados do template
      labels: # labels do template
        app: nginx # label que identifica o app
      annotations: # annotations do template
        prometheus.io/scrape: 'true' # habilita o scraping do Prometheus
        prometheus.io/port: '9113' # porta do target
    spec: # especificação do template
      containers: # containers do template 
        - name: nginx # nome do container
          image: nginx # imagem do container do Nginx
          ports: # portas do container
            - containerPort: 80 # porta do container
              name: http # nome da porta
          volumeMounts: # volumes que serão montados no container
            - name: nginx-config # nome do volume
              mountPath: /etc/nginx/conf.d/default.conf # caminho de montagem do volume
              subPath: nginx.conf # subpath do volume
        - name: nginx-exporter # nome do container que será o exporter
          image: 'nginx/nginx-prometheus-exporter:0.11.0' # imagem do container do exporter
          args: # argumentos do container
            - '-nginx.scrape-uri=http://localhost/metrics' # argumento para definir a URI de scraping
          resources: # recursos do container
            limits: # limites de recursos
              memory: 128Mi # limite de memória
              cpu: 0.3 # limite de CPU
          ports: # portas do container
            - containerPort: 9113 # porta do container que será exposta
              name: metrics # nome da porta
      volumes: # volumes do template
        - configMap: # configmap do volume, nós iremos criar esse volume através de um configmap
            defaultMode: 420 # modo padrão do volume
            name: nginx-config # nome do configmap
          name: nginx-config # nome do volume
Agora vamos entender o que está acontecendo no nosso arquivo YAML.

apiVersion: Versão da API do Kubernetes que estamos utilizando.
kind: Tipo de objeto que estamos criando.
metadata: Informações sobre o objeto que estamos criando.
metadata.name: Nome do nosso objeto.
spec: Especificações do nosso objeto.
spec.selector: Selector que o ServiceMonitor irá utilizar para encontrar os serviços que ele irá monitorar.
spec.selector.matchLabels: Labels que o ServiceMonitor irá utilizar para encontrar os serviços que ele irá monitorar.
spec.selector.matchLabels.app: Label que o ServiceMonitor irá utilizar para encontrar os serviços que ele irá monitorar.
spec.replicas: Quantidade de réplicas que o nosso Deployment irá criar.
spec.template: Template que o nosso Deployment irá utilizar para criar os pods.
spec.template.metadata: Informações sobre o nosso pod.
spec.template.metadata.labels: Labels que serão adicionadas ao nosso pod.
spec.template.metadata.labels.app: Label que será adicionada ao nosso pod.
spec.template.metadata.annotations: Annotations que serão adicionadas ao nosso pod.
spec.template.metadata.annotations.prometheus.io/scrape: Annotation que será adicionada ao nosso pod.
spec.template.metadata.annotations.prometheus.io/port: Annotation que será adicionada ao nosso pod.
spec.template.spec: Especificações do nosso pod.
spec.template.spec.containers: Containers que serão criados no nosso pod.
spec.template.spec.containers.name: Nome do nosso container.
spec.template.spec.containers.image: Imagem que será utilizada no nosso container.
spec.template.spec.containers.ports: Portas que serão expostas no nosso container.
spec.template.spec.containers.ports.containerPort: Porta que será exposta no nosso container.
spec.template.spec.containers.volumeMounts: Volumes que serão montados no nosso container.
spec.template.spec.containers.volumeMounts.name: Nome do volume que será montado no nosso container.
spec.template.spec.containers.volumeMounts.mountPath: Caminho que o volume será montado no nosso container.
spec.template.spec.containers.volumeMounts.subPath.nginx.conf: Subpath que o volume será montado no nosso container.
spec.template.spec.volumes: Volumes que serão criados no nosso pod.
spec.template.spec.volumes.configMap: ConfigMap que será utilizado no nosso volume.
spec.template.spec.volumes.configMap.defaultMode: Modo de permissão que o volume será criado.
spec.template.spec.volumes.configMap.name: Nome do ConfigMap que será utilizado no nosso volume.
spec.template.spec.volumes.name: Nome do nosso volume.
Agora que já sabemos o que está acontecendo no nosso arquivo YAML, vamos criar o nosso Deployment com o seguinte comando:

kubectl apply -f nginx-deployment.yaml
Após o nosso Deployment ser criado, vamos verificar se o nosso pod está rodando:

kubectl get pods
Podemos ver o deployment que acabamos de criar através do comando:

kubectl get deployments
Agora o que precisamos é criar um Service para expor o nosso deployment. Vamos criar o nosso Service com o seguinte arquivo YAML:

apiVersion: v1 # versão da API
kind: Service # tipo de recurso, no caso, um Service
metadata: # metadados do recurso
  name: nginx-svc # nome do recurso
  labels: # labels do recurso
    app: nginx # label para identificar o svc
spec: # especificação do recurso
  ports: # definição da porta do svc 
  - port: 9113 # porta do svc
    name: metrics # nome da porta
  selector: # seletor para identificar os pods/deployment que esse svc irá expor
    app: nginx # label que identifica o pod/deployment que será exposto
Agora vamos entender o que está acontecendo no nosso arquivo YAML.

apiVersion: Versão da API do Kubernetes que estamos utilizando.
kind: Tipo de objeto que estamos criando.
metadata: Informações sobre o objeto que estamos criando.
metadata.name: Nome do nosso objeto.
spec: Especificações do nosso objeto.
spec.selector: Selector que o Service irá utilizar para encontrar os pods que ele irá expor.
spec.selector.app: Label que o Service irá utilizar para encontrar os pods que ele irá expor.
spec.ports: Configurações das portas que serão expostas no nosso Service.
spec.ports.protocol: Protocolo que será utilizado na porta que será exposta.
spec.ports.port: Porta que será exposta no nosso Service.
spec.ports.name: Nome da porta que será exposta no nosso Service.
Vamos criar o Service com o seguinte comando:

kubectl apply -f nginx-service.yaml
Após o nosso Service ser criado, vamos verificar se o nosso Service está rodando:

kubectl get services
Pronto, tudo criado!

Acho que já temos tudo criado, agora vamos verificar se o nosso Nginx está rodando e se as métricas estão sendo expostas.

Vamos verificar se o nosso Nginx está rodando com o seguinte comando:

curl http://<EXTERNAL-IP-DO-SERVICE>:80
Vamos verificar se as métricas do Nginx estão sendo expostas com o seguinte comando:

curl http://<EXTERNAL-IP-DO-SERVICE>:80/nginx_status
Vamos verificar se as métricas do Nginx Exporter estão sendo expostas com o seguinte comando:

curl http://<EXTERNAL-IP-DO-SERVICE>:80/metrics
Ótimo, agora você já sabe como que faz para criar um Service no Kubernetes e expor as métricas do Nginx e do Nginx Exporter. 😄

======================================================================================
Validar as metrics

Primeiros criamos o service.

apiVersion: v1
kind: Service
metadata:
  name: nginx-svc
  labels:
    app: nginx
spec:
  ports:
  - port: 9113
    name: metrics
  selector:
    app: nginx

Segundo criamos o servicemonitor

apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: nginx-servicemonitor
  labels:
    app: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  endpoints:
    - interval: 10s
      path: /metrics
      targetPort: 9113

Terceiro criamos o confimap do volume q devemos usar

apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-config
data:
  nginx.conf: |
    server {
      listen 80;
      location / {
        root /usr/share/nginx/html;
        index index.html index.htm;
      }
      location /metrics {
        stub_status on;
        access_log off;
      }
    }

Por fim criamos o deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-server
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '9113'
        sidecar.istio.io/inject: 'false'  # Disable Istio sidecar injection
    spec:
      containers:
        - name: nginx
          image: nginx
          ports:
            - containerPort: 80
              name: http
          volumeMounts:
            - name: nginx-config
              mountPath: /etc/nginx/conf.d/default.conf
              subPath: nginx.conf
        - name: nginx-exporter
          image: nginx/nginx-prometheus-exporter:0.11.0
          args:
            - '-nginx.scrape-uri=http://localhost/metrics'
          resources:
            limits:
              memory: 128Mi
              cpu: 0.3
          ports:
            - containerPort: 9113
              name: metrics
      volumes:
        - configMap:
            defaultMode: 420
            name: nginx-config
          name: nginx-config
