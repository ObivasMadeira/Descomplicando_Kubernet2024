Dia vamos iniciar falando de pods, a menor unidade no k8s

Comandos:

Comando para pegar tds os pods em execução
kubectl get pods -A

Pega os podes de um determinado namespace, nesse caso usamos o kube-system
kubectl get pods -n kube-system

Comando para pegar mais informações sobre o pod.
kubectl describe nomedopods

COmando para informações do pod

kubectl get pods -o wide

====================== 
vamos iniciar um cluster usando kind usando um arquivo yaml
kind create cluster --config kind-cluster.yaml --name obivas

Após criar o cluster usando kind vamos subir um pod
kubectl run obi01 --image nginx --port 80

Vamos ver o pod e suas informações
kubectl get pods -o wide
Ou tb 
k get pods obi01 -o yaml

Comando para saber qual image esta sendo usando no container dentro do pod
kubectl describe pod obi02 | grep -i image
oot@obi-System-Product-Name:~/k8s/day1/kind# kubectl describe pod obi02 | grep -i image
    Image:          busybox
    Image ID:       docker.io/library/busybox@sha256:5eef5ed34e1e1ff0a4ae850395cbf665c4de6b4b83a32a0bc7bcb998e24e7bbb
  Normal  Pulled     5m7s                  kubelet            Successfully pulled image "busybox" in 1.399126532s
  Normal  Pulling    3m51s (x2 over 5m8s)  kubelet            Pulling image "busybox"
  Normal  Pulled     3m49s                 kubelet            Successfully pulled image "busybox" in 1.698546922s

Comando para acessar o container criado dentro do pod
kubectl attach obi03 -c obi03 -i -t, nesse exemplo ao acessar o mesmo e sair, o container morrer e volta zerado, não salvanod nada o que foi feito dentro dele

Comando para acesso o container criado no pod
kubectl exec -ti obi01 -- bash
====================================================
Subindo dois container com o pod.yaml

apiVersion: v1
kind: Pod
metadata:
  labels:
    run: obi01
    service: webserver
  name: obi01
spec:
  containers:
  - image: nginx:latest
    name: nginx
    resources: {}
  - image: httpd
    name: apache
  dnsPolicy: ClusterFirst
  restartPolicy: Always

Ao tentar subir os dois containers foi apresentado erro.
Para tentar saber qual erro estava apresentando usando os comando:
kubectl logs obi01

Defaulted container "nginx" out of: nginx, apache
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: Looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
10-listen-on-ipv6-by-default.sh: info: Getting the checksum of /etc/nginx/conf.d/default.conf
10-listen-on-ipv6-by-default.sh: info: Enabled listen on IPv6 in /etc/nginx/conf.d/default.conf
/docker-entrypoint.sh: Sourcing /docker-entrypoint.d/15-local-resolvers.envsh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Launching /docker-entrypoint.d/30-tune-worker-processes.sh
/docker-entrypoint.sh: Configuration complete; ready for start up
2024/05/17 15:36:07 [notice] 1#1: using the "epoll" event method
2024/05/17 15:36:07 [notice] 1#1: nginx/1.25.5
2024/05/17 15:36:07 [notice] 1#1: built by gcc 12.2.0 (Debian 12.2.0-14) 
2024/05/17 15:36:07 [notice] 1#1: OS: Linux 6.5.0-35-generic
2024/05/17 15:36:07 [notice] 1#1: getrlimit(RLIMIT_NOFILE): 1048576:1048576
2024/05/17 15:36:07 [notice] 1#1: start worker processes
2024/05/17 15:36:07 [notice] 1#1: start worker process 35

Mas ainda assim não foi possível detectar o real motivo do erro, foi necessário mais analise desse caso, usamos o comando:

kubectl logs obi01 -c apache (usamos o nome do cointainer que esta no yaml, nessa caso o apache)
AH00558: httpd: Could not reliably determine the server's fully qualified domain name, using 10.244.1.5. Set the 'ServerName' directive globally to suppress this message
(98)Address already in use: AH00072: make_sock: could not bind to address [::]:80
(98)Address already in use: AH00072: make_sock: could not bind to address 0.0.0.0:80
no listening sockets available, shutting down
AH00015: Unable to open logs

O motivo do erro foi que estamos tentando subir dois serviços com a mesma porta, nesse caso 80.

===================================================
Criando um pods com limites de recursos

yaml

apiVersion: v1
kind: Pod
metadata:
   labels:
     run: obivas01
   name: obivas01
spec:
  containers:
  - image: ubuntu
    name: ubuntu
    - sleep
    - "1800"
    resources:
      limits:
        memory: "64Mi"
        cpu: "0.5"
      requests:
        memory: "32Mi"
        cpu: "0.3"
  dnsPolicy: ClusterFirst
  restartPolicy: Always


kubectl describe pods obivas01
Name:             obivas01
Namespace:        default
Priority:         0
Service Account:  default
Node:             kind-multiplosnodes-worker3/172.18.0.2
Start Time:       Tue, 21 May 2024 11:18:43 -0300
Labels:           run=obivas01
Annotations:      <none>
Status:           Running
IP:               10.244.3.2
IPs:
  IP:  10.244.3.2
Containers:
  ubuntu:
    Container ID:  containerd://3cda6fb7b25d375fd34c9ca6442216c2e32e30f2edd2400b5c47ceb3d7e8d2d2
    Image:         ubuntu
    Image ID:      docker.io/library/ubuntu@sha256:3f85b7caad41a95462cf5b787d8a04604c8262cdcdf9a472b8c52ef83375fe15
    Port:          <none>
    Host Port:     <none>
    Args:
      sleep
      1800
    State:          Running
      Started:      Tue, 21 May 2024 11:18:50 -0300
    Ready:          True
    Restart Count:  0
    Limits:
      cpu:     500m
      memory:  64Mi
    Requests:
      cpu:        300m
      memory:     32Mi
    Environment:  <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-bdvxn (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-bdvxn:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   Burstable
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  10m   default-scheduler  Successfully assigned default/obivas01 to kind-multiplosnodes-worker3
  Normal  Pulling    10m   kubelet            Pulling image "ubuntu"
  Normal  Pulled     10m   kubelet            Successfully pulled image "ubuntu" in 6.016027927s
  Normal  Created    10m   kubelet            Created container ubuntu
  Normal  Started    10m   kubelet            Started container ubuntu

Acessar o o container dentro do pode

kubectl exec -ti obivas01 -- bash
vamos instalar a ferramenta stress dentro do container criado para validar os "limits" configurado.
apt install stress
vamos iniciar o teste com memória
stress --vm-bytes 64 --vm 1

root@obivas01:/# ps -ef
UID          PID    PPID  C STIME TTY          TIME CMD
root           1       0  0 14:18 ?        00:00:00 sleep 1800
root          13       0  0 14:31 pts/0    00:00:00 bash
root         174      13  0 14:36 pts/0    00:00:00 stress --vm-bytes 64 --vm 1
root         175     174 50 14:36 pts/0    00:00:31 stress --vm-bytes 64 --vm 1
root         176       0  0 14:37 pts/1    00:00:00 bash
root         186     176  0 14:37 pts/1    00:00:00 ps -ef
root@obivas01:/#


