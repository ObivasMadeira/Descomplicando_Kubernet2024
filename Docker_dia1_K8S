Primeiro dia sobre K8S

Esse treinamento foi desenhado para capacitar a pessoa estudante ou a pessoa profissional de TI a trabalhar com o Kubernetes em ambientes criticos.

O Treinamento √© composto por material escrito, aulas gravadas em v√≠deo e aulas ao vivo. Durante o treinamento a pessoa ser√° testada de forma pr√°tica, sendo necess√°rio completar desafios reais para dar continuidade no treinamento.

O foco do treinamento √© capacitar a pessoa para trabalhar com Kubernetes de maneira eficiente e totalmente preparada para trabalhar em ambientes cr√≠ticos que utilizam containers.


===============================================================

Agora vamos iniciar a jornada no K8S

Durante o Day-1 n√≥s vamos entender o que √© um container, vamos falar sobre a import√¢ncia do container runtime e do container engine. 
Durante o Day-1 vamos entender o que √© o Kubernetes e sua arquitetura, vamos falar sobre o control plane, workers, apiserver, scheduler, controller e muito mais! 
Ser√° aqui que iremos criar o nosso primeiro cluster Kubernetes e realizar o deploy de um pod do Nginx. O Day-1 √© para que eu possa me sentir mais confort√°vel com o Kubernetes e seus conceitos iniciais.  

=================================================================
O que √© o Kubernetes?

Vers√£o resumida:

O projeto Kubernetes foi desenvolvido pela Google, em meados de 2014, para atuar como um orquestrador de cont√™ineres para a empresa. O Kubernetes (k8s), cujo termo em Grego significa "timoneiro", √© um projeto open source que conta com design e desenvolvimento baseados no projeto Borg, que tamb√©m √© da Google 1. Alguns outros produtos dispon√≠veis no mercado, tais como o Apache Mesos e o Cloud Foundry, tamb√©m surgiram a partir do projeto Borg.

Como Kubernetes √© uma palavra dif√≠cil de se pronunciar - e de se escrever - a comunidade simplesmente o apelidou de k8s, seguindo o padr√£o i18n (a letra "k" seguida por oito letras e o "s" no final), pronunciando-se simplesmente "kates".

Vers√£o longa:

Praticamente todo software desenvolvido na Google √© executado em cont√™iner 2. A Google j√° gerencia cont√™ineres em larga escala h√° mais de uma d√©cada, quando n√£o se falava tanto sobre isso. Para atender a demanda interna, alguns desenvolvedores do Google constru√≠ram tr√™s sistemas diferentes de gerenciamento de cont√™ineres: Borg, Omega e Kubernetes. Cada sistema teve o desenvolvimento bastante influenciado pelo antecessor, embora fosse desenvolvido por diferentes raz√µes.

O primeiro sistema de gerenciamento de cont√™ineres desenvolvido no Google foi o Borg, constru√≠do para gerenciar servi√ßos de longa dura√ß√£o e jobs em lote, que anteriormente eram tratados por dois sistemas: Babysitter e Global Work Queue. O √∫ltimo influenciou fortemente a arquitetura do Borg, mas estava focado em execu√ß√£o de jobs em lote. O Borg continua sendo o principal sistema de gerenciamento de cont√™ineres dentro do Google por causa de sua escala, variedade de recursos e robustez extrema.

O segundo sistema foi o Omega, descendente do Borg. Ele foi impulsionado pelo desejo de melhorar a engenharia de software do ecossistema Borg. Esse sistema aplicou muitos dos padr√µes que tiveram sucesso no Borg, mas foi constru√≠do do zero para ter a arquitetura mais consistente. Muitas das inova√ß√µes do Omega foram posteriormente incorporadas ao Borg.

O terceiro sistema foi o Kubernetes. Concebido e desenvolvido em um mundo onde desenvolvedores externos estavam se interessando em cont√™ineres e o Google desenvolveu um neg√≥cio em amplo crescimento atualmente, que √© a venda de infraestrutura de nuvem p√∫blica.

O Kubernetes √© de c√≥digo aberto - em contraste com o Borg e o Omega que foram desenvolvidos como sistemas puramente internos do Google. O Kubernetes foi desenvolvido com um foco mais forte na experi√™ncia de desenvolvedores que escrevem aplicativos que s√£o executados em um cluster: seu principal objetivo √© facilitar a implanta√ß√£o e o gerenciamento de sistemas distribu√≠dos, enquanto se beneficia do melhor uso de recursos de mem√≥ria e processamento que os cont√™ineres possibilitam.

Estas informa√ß√µes foram extra√≠das e adaptadas deste artigo, que descreve as li√ß√µes aprendidas com o desenvolvimento e opera√ß√£o desses sistemas.  
Arquitetura do k8s

Assim como os demais orquestradores dispon√≠veis, o k8s tamb√©m segue um modelo control plane/workers, constituindo assim um cluster, onde para seu funcionamento √© recomendado no m√≠nimo tr√™s n√≥s: o n√≥ control-plane, respons√°vel (por padr√£o) pelo gerenciamento do cluster, e os demais como workers, executores das aplica√ß√µes que queremos executar sobre esse cluster.

√â poss√≠vel criar um cluster Kubernetes rodando em apenas um n√≥, por√©m √© recomendado somente para fins de estudos e nunca executado em ambiente produtivo.

Caso voc√™ queira utilizar o Kubernetes em sua m√°quina local, em seu desktop, existem diversas solu√ß√µes que ir√£o criar um cluster Kubernetes, utilizando m√°quinas virtuais ou o Docker, por exemplo.

Com isso voc√™ poder√° ter um cluster Kubernetes com diversos n√≥s, por√©m todos eles rodando em sua m√°quina local, em seu desktop.

Alguns exemplos s√£o:

    Kind: Uma ferramenta para execu√ß√£o de cont√™ineres Docker que simulam o funcionamento de um cluster Kubernetes. √â utilizado para fins did√°ticos, de desenvolvimento e testes. O Kind n√£o deve ser utilizado para produ√ß√£o;

    Minikube: ferramenta para implementar um cluster Kubernetes localmente com apenas um n√≥. Muito utilizado para fins did√°ticos, de desenvolvimento e testes. O Minikube n√£o deve ser utilizado para produ√ß√£o;

    MicroK8S: Desenvolvido pela Canonical, mesma empresa que desenvolve o Ubuntu. Pode ser utilizado em diversas distribui√ß√µes e pode ser utilizado em ambientes de produ√ß√£o, em especial para Edge Computing e IoT (Internet of things);

    k3s: Desenvolvido pela Rancher Labs, √© um concorrente direto do MicroK8s, podendo ser executado inclusive em Raspberry Pi;

    k0s: Desenvolvido pela Mirantis, mesma empresa que adquiriu a parte enterprise do Docker. √â uma distribui√ß√£o do Kubernetes com todos os recursos necess√°rios para funcionar em um √∫nico bin√°rio, que proporciona uma simplicidade na instala√ß√£o e manuten√ß√£o do cluster. A pron√∫ncia √© correta √© kay-zero-ess e tem por objetivo reduzir o esfor√ßo t√©cnico e desgaste na instala√ß√£o de um cluster Kubernetes, por isso o seu nome faz alus√£o a Zero Friction. O k0s pode ser utilizado em ambientes de produ√ß√£o;

    API Server: √â um dos principais componentes do k8s. Este componente fornece uma API que utiliza JSON sobre HTTP para comunica√ß√£o, onde para isto √© utilizado principalmente o utilit√°rio kubectl, por parte dos administradores, para a comunica√ß√£o com os demais n√≥s, como mostrado no gr√°fico. Estas comunica√ß√µes entre componentes s√£o estabelecidas atrav√©s de requisi√ß√µes REST;

    etcd: O etcd √© um datastore chave-valor distribu√≠do que o k8s utiliza para armazenar as especifica√ß√µes, status e configura√ß√µes do cluster. Todos os dados armazenados dentro do etcd s√£o manipulados apenas atrav√©s da API. Por quest√µes de seguran√ßa, o etcd √© por padr√£o executado apenas em n√≥s classificados como control plane no cluster k8s, mas tamb√©m podem ser executados em clusters externos, espec√≠ficos para o etcd, por exemplo;

    Scheduler: O scheduler √© respons√°vel por selecionar o n√≥ que ir√° hospedar um determinado pod (a menor unidade de um cluster k8s - n√£o se preocupe sobre isso por enquanto, n√≥s falaremos mais sobre isso mais tarde) para ser executado. Esta sele√ß√£o √© feita baseando-se na quantidade de recursos dispon√≠veis em cada n√≥, como tamb√©m no estado de cada um dos n√≥s do cluster, garantindo assim que os recursos sejam bem distribu√≠dos. Al√©m disso, a sele√ß√£o dos n√≥s, na qual um ou mais pods ser√£o executados, tamb√©m pode levar em considera√ß√£o pol√≠ticas definidas pelo usu√°rio, tais como afinidade, localiza√ß√£o dos dados a serem lidos pelas aplica√ß√µes, etc;

    Controller Manager: √â o controller manager quem garante que o cluster esteja no √∫ltimo estado definido no etcd. Por exemplo: se no etcd um deploy est√° configurado para possuir dez r√©plicas de um pod, √© o controller manager quem ir√° verificar se o estado atual do cluster corresponde a este estado e, em caso negativo, procurar√° conciliar ambos;

    Kubelet: O kubelet pode ser visto como o alente do k8s que √© executado nos n√≥s workers. Em cada n√≥ worker dever√° existir um agente Kubelet em execu√ß√£o. O Kubelet √© respons√°vel por de fato gerenciar os pods que foram direcionados pelo controller do cluster, dentro dos n√≥s, de forma que para isto o Kubelet pode iniciar, parar e manter os cont√™ineres e os pods em funcionamento de acordo com o instru√≠do pelo controlador do cluster;

    Kube-proxy: Age como um proxy e um load balancer. Este componente √© respons√°vel por efetuar roteamento de requisi√ß√µes para os pods corretos, como tamb√©m por cuidar da parte de rede do n√≥;

 
Portas que devemos nos preocupar

CONTROL PLANE
Protocol 	Direction 	Port Range 	Purpose 	Used By
TCP 	Inbound 	6443* 	Kubernetes API server 	All
TCP 	Inbound 	2379-2380 	etcd server client API 	kube-apiserver, etcd
TCP 	Inbound 	10250 	Kubelet API 	Self, Control plane
TCP 	Inbound 	10251 	kube-scheduler 	Self
TCP 	Inbound 	10252 	kube-controller-manager 	Self

    Toda porta marcada por * √© customiz√°vel, voc√™ precisa se certificar que a porta alterada tamb√©m esteja aberta.

  WORKERS
Protocol 	Direction 	Port Range 	Purpose 	Used By
TCP 	Inbound 	10250 	Kubelet API 	Self, Control plane
TCP 	Inbound 	30000-32767 	NodePort 	Services All

 
Conceitos-chave do k8s

√â importante saber que a forma como o k8s gerencia os cont√™ineres √© ligeiramente diferente de outros orquestradores, como o Docker Swarm, sobretudo devido ao fato de que ele n√£o trata os cont√™ineres diretamente, mas sim atrav√©s de pods. Vamos conhecer alguns dos principais conceitos que envolvem o k8s a seguir:

    Pod: √â o menor objeto do k8s. Como dito anteriormente, o k8s n√£o trabalha com os cont√™ineres diretamente, mas organiza-os dentro de pods, que s√£o abstra√ß√µes que dividem os mesmos recursos, como endere√ßos, volumes, ciclos de CPU e mem√≥ria. Um pod pode possuir v√°rios cont√™ineres;

    Deployment: √â um dos principais controllers utilizados. O Deployment, em conjunto com o ReplicaSet, garante que determinado n√∫mero de r√©plicas de um pod esteja em execu√ß√£o nos n√≥s workers do cluster. Al√©m disso, o Deployment tamb√©m √© respons√°vel por gerenciar o ciclo de vida das aplica√ß√µes, onde caracter√≠sticas associadas a aplica√ß√£o, tais como imagem, porta, volumes e vari√°veis de ambiente, podem ser especificados em arquivos do tipo yaml ou json para posteriormente serem passados como par√¢metro para o kubectl executar o deployment. Esta a√ß√£o pode ser executada tanto para cria√ß√£o quanto para atualiza√ß√£o e remo√ß√£o do deployment;

    ReplicaSets: √â um objeto respons√°vel por garantir a quantidade de pods em execu√ß√£o no n√≥;

    Services: √â uma forma de voc√™ expor a comunica√ß√£o atrav√©s de um ClusterIP, NodePort ou LoadBalancer para distribuir as requisi√ß√µes entre os diversos Pods daquele Deployment. Funciona como um balanceador de carga.

Instalando e customizando o Kubectl
Instala√ß√£o do Kubectl no GNU/Linux

Vamos instalar o kubectl com os seguintes comandos.

curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client

==================================================

Kind

O Kind (Kubernetes in Docker) √© outra alternativa para executar o Kubernetes num ambiente local para testes e aprendizado, mas n√£o √© recomendado para uso em produ√ß√£o.
Instala√ß√£o no GNU/Linux

Para fazer a instala√ß√£o no GNU/Linux, execute os seguintes comandos.

curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64

chmod +x ./kind

sudo mv ./kind /usr/local/bin/kind

 
Instala√ß√£o no MacOS

Para fazer a instala√ß√£o no MacOS, execute o seguinte comando.

sudo brew install kind

  ou

curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-darwin-amd64
chmod +x ./kind
mv ./kind /usr/bin/kind

=============================================================

Criando um cluster com o Kind

Ap√≥s realizar a instala√ß√£o do Kind, vamos iniciar o nosso cluster.

kind create cluster

Creating cluster "kind" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Not sure what to do next? üòÖ  Check out https://kind.sigs.k8s.io/docs/user/quick-start/

  √â poss√≠vel criar mais de um cluster e personalizar o seu nome.

kind create cluster --name obivas

Creating cluster "obivas" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
Set kubectl context to "kind-obivas"
You can now use your cluster with:

kubectl cluster-info --context kind-obivas

Thanks for using kind! üòä

  Para visualizar os seus clusters utilizando o kind, execute o comando a seguir.

kind get clusters

  Liste os nodes do cluster.

kubectl get nodes

 
Criando um cluster com m√∫ltiplos n√≥s locais com o Kind

√â poss√≠vel para essa aula incluir m√∫ltiplos n√≥s na estrutura do Kind, que foi mencionado anteriormente.

Execute o comando a seguir para selecionar e remover todos os clusters locais criados no Kind.

kind delete clusters $(kind get clusters)

Deleted clusters: ["obivas" "kind"]

  Crie um arquivo de configura√ß√£o para definir quantos e o tipo de n√≥s no cluster que voc√™ deseja. No exemplo a seguir, ser√° criado o arquivo de configura√ß√£o kind-3nodes.yaml para especificar um cluster com 1 n√≥ control-plane (que executar√° o control plane) e 2 workers.

cat << EOF > $HOME/kind-3nodes.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF


