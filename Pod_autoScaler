/O que iremos ver hoje?

Hoje é um dia particularmente fascinante! Vamos desbravar os territórios do Kubernetes, explorando a magia do Horizontal Pod Autoscaler (HPA),
uma ferramenta indispensável para quem almeja uma operação eficiente e resiliente. Portanto, afivelem os cintos e preparem-se para uma jornada de descobertas. 
A aventura #VAIIII começar! hahahha

===============================

Introdução ao Horizontal Pod Autoscaler (HPA)

O Horizontal Pod Autoscaler, carinhosamente conhecido como HPA, é uma das joias brilhantes incrustadas no coração do Kubernetes. Com o HPA, podemos ajustar automaticamente o número de réplicas de um conjunto de pods,
assegurando que nosso aplicativo tenha sempre os recursos necessários para performar eficientemente, sem desperdiçar recursos.
O HPA é como um maestro que, com a batuta das métricas, rege a orquestra de pods, 
assegurando que a harmonia seja mantida mesmo quando a sinfonia do tráfego de rede atinge seu crescendo.
Como o HPA Funciona?
O HPA é o olheiro vigilante que monitora as métricas dos nossos pods.
A cada batida do seu coração métrico, que ocorre em intervalos regulares,
ele avalia se os pods estão suando a camisa para atender às demandas ou se estão relaxando mais do que deveriam. Com base nessa avaliação,
ele toma a decisão sábia de convocar mais soldados para o campo de batalha ou de dispensar alguns para um merecido descanso.

Certamente! O Metrics Server é uma componente crucial para o funcionamento do Horizontal Pod Autoscaler (HPA), pois fornece as métricas necessárias para que o HPA tome decisões de escalonamento.
Vamos entender um pouco mais sobre o Metrics Server e como instalá-lo em diferentes ambientes Kubernetes, incluindo Minikube e KinD.

=========================================
Vamos primeramente instalar o metricserver para usarmos o HPA

Introdução ao Metrics Server

Antes de começarmos a explorar o Horizontal Pod Autoscaler (HPA), é essencial termos o Metrics Server instalado em nosso cluster Kubernetes. O Metrics Server é um agregador de métricas de recursos de sistema, que coleta métricas como uso de CPU e memória dos nós e pods no cluster. Essas métricas são vitais para o funcionamento do HPA, pois são usadas para determinar quando e como escalar os recursos.
Por que o Metrics Server é importante para o HPA?
O HPA utiliza métricas de uso de recursos para tomar decisões inteligentes sobre o escalonamento dos pods. Por exemplo, se a utilização da CPU de um pod exceder um determinado limite, o HPA pode decidir aumentar o número de réplicas desse pod. Da mesma forma, se a utilização da CPU for muito baixa, o HPA pode decidir reduzir o número de réplicas. Para fazer isso de forma eficaz, o HPA precisa ter acesso a métricas precisas e atualizadas, que são fornecidas pelo Metrics Server. Portanto, precisamos antes conhecer essa peça fundamental para o dia de hoje! :D
Instalando o Metrics Server
No Amazon EKS e na maioria dos clusters Kubernetes
Durante a nossa aula, estou com um cluster EKS, e para instalar o Metrics Server, podemos usar o seguinte comando:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
Esse comando aplica o manifesto do Metrics Server ao seu cluster, instalando todos os componentes necessários.
No Minikube:
A instalação do Metrics Server no Minikube é bastante direta. Use o seguinte comando para habilitar o Metrics Server:
minikube addons enable metrics-server
Após a execução deste comando, o Metrics Server será instalado e ativado em seu cluster Minikube.
No KinD (Kubernetes in Docker):
Para o KinD, você pode usar o mesmo comando que usou para o EKS:
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
Verificando a Instalação do Metrics Server
Após a instalação do Metrics Server, é uma boa prática verificar se ele foi instalado corretamente e está funcionando como esperado. Execute o seguinte comando para obter a lista de pods no namespace kube-system e verificar se o pod do Metrics Server está em execução:
kubectl get pods -n kube-system | grep metrics-server
Obtendo Métricas
Com o Metrics Server em execução, agora você pode começar a coletar métricas de seu cluster. Aqui está um exemplo de como você pode obter métricas de uso de CPU e memória para todos os seus nodes:
kubectl top nodes
E para obter métricas de uso de CPU e memória para todos os seus pods:
kubectl top pods
Esses comandos fornecem uma visão rápida da utilização de recursos em seu cluster, o que é crucial para entender e otimizar o desempenho de seus aplicativos.



kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/high-availability-1.21+.yaml

k get pods -n kube-system
NAME                                           READY   STATUS    RESTARTS      AGE
coredns-6d4b75cb6d-dfmj6                       1/1     Running   0             58m
coredns-6d4b75cb6d-tqmrj                       1/1     Running   0             58m
etcd-obivas-control-plane                      1/1     Running   0             25h
kindnet-2r89m                                  1/1     Running   2 (25h ago)   7d
kindnet-5sd2z                                  1/1     Running   2             7d
kindnet-7zkrz                                  1/1     Running   2 (25h ago)   7d
kindnet-jdvwf                                  1/1     Running   2 (25h ago)   7d
kindnet-r2qxf                                  1/1     Running   2 (25h ago)   7d
kube-apiserver-obivas-control-plane            1/1     Running   0             25h
kube-controller-manager-obivas-control-plane   1/1     Running   8 (25h ago)   6d
kube-proxy-8f9rw                               1/1     Running   0             59m
kube-proxy-cc7g2                               1/1     Running   0             59m
kube-proxy-dvpgx                               1/1     Running   0             59m
kube-proxy-q7zqr                               1/1     Running   0             59m
kube-proxy-vgzst                               1/1     Running   0             59m
kube-scheduler-obivas-control-plane            1/1     Running   5 (25h ago)   6d
metrics-server-6c8776dd7f-td84s                1/1     Running   0             5m13s
metrics-server-6c8776dd7f-tp95r                1/1     Running   0             5m11s


kubectl top pods
NAME                               CPU(cores)   MEMORY(bytes)   
giropops-senhas-b887fccdc-gp7sd    1m           44Mi            
giropops-senhas-b887fccdc-np8pc    1m           45Mi            
giropops-senhas-b887fccdc-s58tb    1m           45Mi            
redis-deployment-764df994c-mwp98   2m           6Mi             
kubectl top nodes
NAME                   CPU(cores)   CPU(%)   MEMORY(bytes)   MEMORY(%)   
obivas-control-plane   63m          0%       935Mi           5%          
obivas-worker          16m          0%       293Mi           1%          
obivas-worker2         13m          0%       201Mi           1%          
obivas-worker3         16m          0%       273Mi           1%          
obivas-worker4         18m          0%       305Mi           1%          
============================================================
Vamos criar um deployment do nginx para testar o HPA
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.27-alpine
        ports:
        - containerPort: 80
          name: http
          protocol: TCP
        resources:
          limits:
            memory: "32Mi"
            cpu: "0.02m"

---
apiVersion: v1
kind: Service
metadata:
  name: nginx-service
  labels:
    app: nginx
spec:
  type: ClusterIP
  ports:
  - port: 80
    targetPort: 80
    protocol: TCP
    name: http
  selector:
    app: nginx

kubectl apply -f nginx_hpa.yaml
kubectl get pods
NAME                                READY   STATUS    RESTARTS      AGE
giropops-senhas-b887fccdc-gp7sd     1/1     Running   3 (17m ago)   6d22h
giropops-senhas-b887fccdc-np8pc     1/1     Running   3 (17m ago)   6d22h
giropops-senhas-b887fccdc-s58tb     1/1     Running   3 (17m ago)   6d22h
nginx-deployment-86d479fdff-xt6pg   1/1     Running   0             66s
redis-deployment-764df994c-mwp98    1/1     Running   3 (17m ago)   6d22h

Agora vamos criar nosso primeiro HPA

# Definição do HPA para o nginx-deployment
apiVersion: autoscaling/v2  # Versão da API que define um HPA
kind: HorizontalPodAutoscaler  # Tipo de recurso que estamos definindo
metadata:
  name: nginx-deployment-hpa  # Nome do nosso HPA
spec:
  scaleTargetRef:
    apiVersion: apps/v1        # A versão da API do recurso alvo
    kind: Deployment           # O tipo de recurso alvo
    name: nginx-deployment     # O nome do recurso alvo
  minReplicas: 3               # Número mínimo de réplicas
  maxReplicas: 5              # Número máximo de réplicas
  metrics:
  - type: Resource             # Tipo de métrica (recurso do sistema)
    resource:
      name: cpu                # Nome da métrica (CPU neste caso)
      target:
        type: Utilization      # Tipo de alvo (utilização)
        averageUtilization: 50 # Valor alvo (50% de utilização)

k get hpa
NAME        REFERENCE                     TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
nginx-hpa   Deployment/nginx-deployment   <unknown>/50%   3         5         1          4m43s

k describe deployment nginx-deployment
Name:                   nginx-deployment
Namespace:              default
CreationTimestamp:      Tue, 18 Nov 2025 20:48:05 -0300
Labels:                 app=nginx
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=nginx
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=nginx
  Containers:
   nginx:
    Image:      nginx:1.27-alpine
    Port:       80/TCP (http)
    Host Port:  0/TCP (http)
    Limits:
      cpu:         1m
      memory:      32Mi
    Environment:   <none>
    Mounts:        <none>
  Volumes:         <none>
  Node-Selectors:  <none>
  Tolerations:     <none>
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Progressing    True    NewReplicaSetAvailable
  Available      True    MinimumReplicasAvailable
OldReplicaSets:  <none>
NewReplicaSet:   nginx-deployment-86d479fdff (3/3 replicas created)
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  20m    deployment-controller  Scaled up replica set nginx-deployment-86d479fdff to 1
  Normal  ScalingReplicaSet  3m12s  deployment-controller  Scaled up replica set nginx-deployment-86d479fdff to 3

k get pods
NAME                                READY   STATUS    RESTARTS      AGE
giropops-senhas-b887fccdc-gp7sd     1/1     Running   3 (36m ago)   6d22h
giropops-senhas-b887fccdc-np8pc     1/1     Running   3 (35m ago)   6d22h
giropops-senhas-b887fccdc-s58tb     1/1     Running   3 (36m ago)   6d22h
nginx-deployment-86d479fdff-hj88m   1/1     Running   0             2m12s
nginx-deployment-86d479fdff-msgsx   1/1     Running   0             2m12s
nginx-deployment-86d479fdff-xt6pg   1/1     Running   0             19m
redis-deployment-764df994c-mwp98    1/1     Running   3 (36m ago)   6d23h

k describe hpa nginx
Name:                                                  nginx-hpa
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Tue, 18 Nov 2025 21:01:09 -0300
Reference:                                             Deployment/nginx-deployment
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  0% (0) / 50%
Min replicas:                                          3
Max replicas:                                          5
Deployment pods:                                       3 current / 3 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  False   DesiredWithinRange   the desired count is within the acceptable range
Events:
  Type     Reason             Age                     From                       Message
  ----     ------             ----                    ----                       -------
  Warning  FailedGetScale     5m46s (x17 over 9m50s)  horizontal-pod-autoscaler  deployments/scale.apps "nginx" not found
  Normal   SuccessfulRescale  5m29s                   horizontal-pod-autoscaler  New size: 3; reason: Current number of replicas below Spec.MinReplicas

Exemplos Práticos com HPA

Agora que você já entende o básico sobre o HPA, é hora de rolar as mangas e entrar na prática. Vamos explorar como o HPA responde a diferentes métricas e cenários.
Autoscaling com base na utilização de CPU

Vamos começar com um exemplo clássico de escalonamento baseado na utilização da CPU, que já discutimos anteriormente. Para tornar a aprendizagem mais interativa, vamos simular um aumento de tráfego e observar como o HPA responde a essa mudança.

kubectl run -i --tty load-generator --image=busybox /bin/sh

while true; do wget -q -O- http://nginx-deployment.default.svc.cluster.local; done

Este script simples cria uma carga constante no nosso deployment, fazendo requisições contínuas ao servidor Nginx. Você poderá observar como o HPA ajusta o número de réplicas para manter a utilização da CPU em torno do limite definido.
Autoscaling com base na utilização de Memória

O HPA não é apenas um mestre em lidar com a CPU, ele também tem um olho afiado para a memória. Vamos explorar como configurar o HPA para escalar baseado na utilização de memória.

# Definição do HPA para escalonamento baseado em memória
apiVersion: autoscaling/v2  # Versão da API que define um HPA
kind: HorizontalPodAutoscaler    # Tipo de recurso que estamos definindo
metadata:
  name: nginx-deployment-hpa-memory  # Nome do nosso HPA
spec:
  scaleTargetRef:
    apiVersion: apps/v1              # A versão da API do recurso alvo
    kind: Deployment                 # O tipo de recurso alvo
    name: nginx-deployment           # O nome do recurso alvo
  minReplicas: 3                     # Número mínimo de réplicas
  maxReplicas: 10                    # Número máximo de réplicas
  metrics:
  - type: Resource                   # Tipo de métrica (recurso do sistema)
    resource:
      name: memory                   # Nome da métrica (memória neste caso)
      target:
        type: Utilization            # Tipo de alvo (utilização)
        averageUtilization: 70       # Valor alvo (70% de utilização)

Neste exemplo, o HPA vai ajustar o número de réplicas para manter a utilização de memória em cerca de 70%. Assim, nosso deployment pode respirar livremente mesmo quando a demanda aumenta.
===========================
OBS:

Tiver que alterar meu ingress para funcionar as duas aplicações q estou usando, giropops-senhas e locuts para teste de cpu, se algum dias quiser usar outro app no mesmo formato é só add
 
host: outra-app.localhost
  http:
    paths:
    - path: /
      pathType: Prefix
      backend:
        service:
          name: nome-do-service
          port:
            number: porta


Segue o novo ingress

# Service para o Locust
apiVersion: v1
kind: Service
metadata:
  name: locust-service
spec:
  selector:
    app: locust-giropops
  ports:
  - protocol: TCP
    port: 8089
    targetPort: 8089
  type: ClusterIP

---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: apps-ingress
spec:
  ingressClassName: nginx
  rules:
  - host: giropops.localhost #giropops-senhas
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: giropops-senhas
            port:
              number: 5000
  - host: locust.localhost
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: locust-giropops
            port:
              number: 8089


=============================================
Após a ferramenta locust funcionar usando ingress, testamos a mesma, e o HPA fez o scale

kubectl describe hpa nginx
Name:                                                  nginx-hpa
Namespace:                                             default
Labels:                                                <none>
Annotations:                                           <none>
CreationTimestamp:                                     Tue, 18 Nov 2025 21:01:09 -0300
Reference:                                             Deployment/nginx-deployment
Metrics:                                               ( current / target )
  resource cpu on pods  (as a percentage of request):  600% (6m) / 50%
Min replicas:                                          3
Max replicas:                                          5
Deployment pods:                                       5 current / 5 desired
Conditions:
  Type            Status  Reason               Message
  ----            ------  ------               -------
  AbleToScale     True    ScaleDownStabilized  recent recommendations were higher than current one, applying the highest recent recommendation
  ScalingActive   True    ValidMetricFound     the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)
  ScalingLimited  True    TooManyReplicas      the desired replica count is more than the maximum replica count
Events:
  Type    Reason             Age    From                       Message
  ----    ------             ----   ----                       -------
  Normal  SuccessfulRescale  3m18s  horizontal-pod-autoscaler  New size: 4; reason: cpu resource utilization (percentage of request) above target
  Normal  SuccessfulRescale  3m3s   horizontal-pod-autoscaler  New size: 5; reason: cpu resource utilization (percentage of request) above target

As configurações que usamos foi 1000 user, 100 Spawn rate (users started/second) e host http://nginx-service

Depois de um tempo ele matou os pods que foram criados

k get pods -w
NAME                                READY   STATUS    RESTARTS       AGE
giropops-senhas-b887fccdc-gp7sd     1/1     Running   3 (124m ago)   7d
giropops-senhas-b887fccdc-np8pc     1/1     Running   3 (124m ago)   7d
giropops-senhas-b887fccdc-s58tb     1/1     Running   3 (124m ago)   7d
locust-giropops-8789f646c-lnnmh     1/1     Running   0              64m
nginx-deployment-86d479fdff-4bt9k   1/1     Running   0              7m38s
nginx-deployment-86d479fdff-hj88m   1/1     Running   0              90m
nginx-deployment-86d479fdff-msgsx   1/1     Running   0              90m
nginx-deployment-86d479fdff-q68q5   1/1     Running   0              7m23s
nginx-deployment-86d479fdff-xt6pg   1/1     Running   0              108m
redis-deployment-764df994c-mwp98    1/1     Running   3 (124m ago)   7d
nginx-deployment-86d479fdff-q68q5   1/1     Terminating   0              10m
nginx-deployment-86d479fdff-q68q5   0/1     Terminating   0              10m
nginx-deployment-86d479fdff-q68q5   0/1     Terminating   0              10m
nginx-deployment-86d479fdff-q68q5   0/1     Terminating   0              10m
nginx-deployment-86d479fdff-4bt9k   1/1     Terminating   0              11m
nginx-deployment-86d479fdff-4bt9k   0/1     Terminating   0              11m
nginx-deployment-86d479fdff-4bt9k   0/1     Terminating   0              11m
nginx-deployment-86d479fdff-4bt9k   0/1     Terminating   0              11m
=====================================================

Autoscaling com base na utilização de Memória

O HPA não é apenas um mestre em lidar com a CPU, ele também tem um olho afiado para a memória. Vamos explorar como configurar o HPA para escalar baseado na utilização de memória.

# Definição do HPA para escalonamento baseado em memória
apiVersion: autoscaling/v2  # Versão da API que define um HPA
kind: HorizontalPodAutoscaler    # Tipo de recurso que estamos definindo
metadata:
  name: nginx-deployment-hpa-memory  # Nome do nosso HPA
spec:
  scaleTargetRef:
    apiVersion: apps/v1              # A versão da API do recurso alvo
    kind: Deployment                 # O tipo de recurso alvo
    name: nginx-deployment           # O nome do recurso alvo
  minReplicas: 3                     # Número mínimo de réplicas
  maxReplicas: 10                    # Número máximo de réplicas
  metrics:
  - type: Resource                   # Tipo de métrica (recurso do sistema)
    resource:
      name: memory                   # Nome da métrica (memória neste caso)
      target:
        type: Utilization            # Tipo de alvo (utilização)
        averageUtilization: 70       # Valor alvo (70% de utilização)

Neste exemplo, o HPA vai ajustar o número de réplicas para manter a utilização de memória em cerca de 70%. Assim, nosso deployment pode respirar livremente mesmo quando a demanda aumenta.

=============================================================

